# AUTOGENERATED! DO NOT EDIT! File to edit: 01_batchbald.ipynb (unless otherwise specified).

# __all__ = ['compute_conditional_entropy', 'compute_entropy', 'CandidateBatch', 'get_batchbald_batch', 'get_bald_batch']
__all__ = ['compute_conditional_entropy', 'compute_entropy', 'CandidateBatch', 'get_batchbald_batch', 'get_bald_batch', 'get_lbb_batch']

# Cell
import math
from dataclasses import dataclass
from typing import List

import torch
from toma import toma
from tqdm.auto import tqdm

from batchbald_redux import joint_entropy

# Cell
import numpy as np
from numpy import savetxt
from numpy import asarray
from numpy import savez_compressed
import random
from dppy.finite_dpps import FiniteDPP
from scipy.optimize import root_scalar

random_seed = 42
random.seed(random_seed)
np.random.seed(random_seed)
torch.manual_seed(random_seed)
torch.backends.cudnn.deterministic = True

def compute_conditional_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    entropies_N = torch.empty(N, dtype=torch.double)

    pbar = tqdm(total=N, desc="Conditional Entropy", leave=False)

    @toma.execute.chunked(log_probs_N_K_C, 1024)
    def compute(log_probs_n_K_C, start: int, end: int):
        nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)

        entropies_N[start:end].copy_(-torch.sum(nats_n_K_C, dim=(1, 2)) / K)
        pbar.update(end - start)

    pbar.close()

    return entropies_N


def compute_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    entropies_N = torch.empty(N, dtype=torch.double)

    pbar = tqdm(total=N, desc="Entropy", leave=False)

    @toma.execute.chunked(log_probs_N_K_C, 1024)
    def compute(log_probs_n_K_C, start: int, end: int):
        mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
        nats_n_C = mean_log_probs_n_C * torch.exp(mean_log_probs_n_C)

        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1))
        pbar.update(end - start)

    pbar.close()

    return entropies_N

# Cell


@dataclass
class CandidateBatch:
    scores: List[float]
    indices: List[int]


def get_batchbald_batch(
    log_probs_N_K_C: torch.Tensor, batch_size: int, num_samples: int, dtype=None, device=None
) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape
    # new probs that will be added to 
    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    if batch_size == 0:
        return CandidateBatch(candidate_scores, candidate_indices)

    conditional_entropies_N = compute_conditional_entropy(log_probs_N_K_C) # from ordinary BALD

    batch_joint_entropy = joint_entropy.DynamicJointEntropy(
        num_samples, batch_size - 1, K, C, dtype=dtype, device=device
    ) # same joint_entr class but with dynamic properties

    # We always keep these on the CPU.
    scores_N = torch.empty(N, dtype=torch.double, pin_memory=torch.cuda.is_available())

    for i in tqdm(range(batch_size), desc="BatchBALD", leave=False):
        if i > 0:
            latest_index = candidate_indices[-1]
            batch_joint_entropy.add_variables(log_probs_N_K_C[latest_index : latest_index + 1])

        shared_conditinal_entropies = conditional_entropies_N[candidate_indices].sum() # they are easier, all sums

        batch_joint_entropy.compute_batch(log_probs_N_K_C, output_entropies_B=scores_N)

        scores_N -= conditional_entropies_N + shared_conditinal_entropies
        scores_N[candidate_indices] = -float("inf")

        candidate_score, candidate_index = scores_N.max(dim=0)

        candidate_indices.append(candidate_index.item())
        candidate_scores.append(candidate_score.item())

    return CandidateBatch(candidate_scores, candidate_indices)

# Cell


def get_bald_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    scores_N += compute_entropy(log_probs_N_K_C)

    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

def get_entropy_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N += compute_entropy(log_probs_N_K_C)

    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

def get_variance_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N += torch.var(log_probs_N_K_C, dim=1).sum(dim=1)

    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())
#####
# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
#     N, K, C = log_probs_N_K_C.shape

#     entropies_N = torch.empty(N, dtype=torch.double)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
# #         print("log_probs_n_K_C.shape:", log_probs_n_K_C.shape)
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
# #         nats_n_C = torch.zeros(N, C) #?
#         n = log_probs_n_K_C.shape[0]
#         nats_n = torch.zeros(n) # N

# #         for i in range(N-1):
#         for i in range(n):
# #             for j in range(i+1, N):
#             for j in range(n): # N can be bigger than batch_size -> error # one of possible solut-s -- tensor view
# #                 print("i, j:", (i, j))
# #                 print("eq:", torch.exp(mean_log_probs_n_C[i][:]) * torch.exp(mean_log_probs_n_C[j][:])*\
# #                 (mean_log_probs_n_C[i][:] + mean_log_probs_n_C[j][:]) - torch.exp(mean_log_probs_n_C[i][:])*\
# #                 mean_log_probs_n_C[i][:] - torch.exp(mean_log_probs_n_C[j][:]) * mean_log_probs_n_C[j][:])
# #                 nats_n_C
#                 if i != j:
# #                     print("i, j:", (i, j))
# #                     print("mean_log_probs_n_C.shape:", mean_log_probs_n_C.shape)
#                     nats_n[i] += torch.sum(torch.exp(mean_log_probs_n_C[i][:]) * torch.exp(mean_log_probs_n_C[j][:])*\
#                     (mean_log_probs_n_C[i][:] + mean_log_probs_n_C[j][:]) - torch.exp(mean_log_probs_n_C[i][:])*\
#                     mean_log_probs_n_C[i][:] - torch.exp(mean_log_probs_n_C[j][:]) * mean_log_probs_n_C[j][:])
# #                 print("nats_n_C:", nats_n_C)

# #                 print("nats_n:", nats_n) ###
    
# #         print("nats_n_C.shape:", nats_n_C.shape)
# #         print("nats_n_C:", nats_n_C)
# #         print("mean_log_probs_n_C:", mean_log_probs_n_C)
#         entropies_N[start:end].copy_(nats_n)
# #         -torch.sum(nats_n_C, dim=1)

# #         print("entropies_N:", entropies_N) ###
    
#         pbar.update(end - start)

#     pbar.close()

#     return entropies_N
#####
# tensor way

# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
#     N, K, C = log_probs_N_K_C.shape

#     entropies_N = torch.empty(N, dtype=torch.double)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
#         n = log_probs_n_K_C.shape[0]
# #         print("1 matmul:", torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t()).shape)
# #         print("2 *:", torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[0]).fill_diagonal_(0.0).shape)
# #         print("3 matmul:", mean_log_probs_n_C.shape)
#         a = torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[0]).fill_diagonal_(0.0), mean_log_probs_n_C).fill_diagonal_(0.0)
#         b = torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[0]).fill_diagonal_(0.0), mean_log_probs_n_C)*torch.eye(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[1])
# #         c = torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t())*torch.eye(mean_log_probs_n_C.shape[0])*n
# #         d = torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t())*torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[0]).fill_diagonal_(0.0)
#         c = torch.exp(mean_log_probs_n_C) * mean_log_probs_n_C * torch.eye(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[1])*(n-1) #*n #*(n-1) #?
#         d = torch.exp(mean_log_probs_n_C) * mean_log_probs_n_C * torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[1]).fill_diagonal_(0.0)
        
# #         print((a * b).shape)
# #         print(c.shape)
# #         print(d.shape)
# #         nats_n = (torch.matmul(a, b.t()) - c - d).sum(dim=1)
#         nats_n = (a + b - c - d).sum(dim=1)

#         pbar.update(end - start)

#     pbar.close()

#     return entropies_N
#####
# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
#     N, K, C = log_probs_N_K_C.shape

#     entropies_N = torch.empty(N, dtype=torch.double)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K) # change
#         n = log_probs_n_K_C.shape[0]
#         a = torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[0]).fill_diagonal_(0.0)
#         b = (a * torch.log(a)).sum(dim=1)
#         c = (torch.matmul(a, mean_log_probs_n_C) * torch.eye(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[1])).sum(dim=1)
#         d = (torch.matmul(a, mean_log_probs_n_C) * torch.ones(mean_log_probs_n_C.shape[0], mean_log_probs_n_C.shape[1]).fill_diagonal_(0.0)).sum(dim=1)

# #         print("a.shape:", a.shape)
# #         print("b.shape:", b.shape)
# #         print("c.shape:", c.shape)
# #         print("d.shape:", d.shape)
#         nats_n = (b - c - d) #.sum(dim=1)

#         pbar.update(end - start)

#     pbar.close()

#     return entropies_N
#####
# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
#     N, K, C = log_probs_N_K_C.shape

#     entropies_N = torch.empty(N, dtype=torch.double)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
# #         nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)
#         n = log_probs_n_K_C.shape[0]
# #         print("1:", torch.exp(log_probs_n_K_C[:, :, :, None]).shape) # .transpose(2, 3)
# #         print("2:", torch.exp(log_probs_n_K_C)[:, :, None, :].shape) # .transpose(1, 2) #.transpose(1, 3)
# #         print("1:", torch.exp(log_probs_n_K_C[:, :, None, :]).shape)
# #         print("2:", torch.exp(log_probs_n_K_C)[:, None, :, :].transpose(2, 3).shape)

#         # mb to use expand func as in joint prob file
#         a = torch.matmul(torch.exp(log_probs_n_K_C[:, :, :, None]), torch.exp(log_probs_n_K_C)[:, :, None, :]) # .transpose(1, 3)
# #         a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 0, 1), torch.exp(log_probs_n_K_C).permute(2, 1, 0))
    
#         print("a.shape:", a.shape)
#         a = a.sum(dim=(1, 2)) / K
#         print("a.shape:", a.shape)
#         a = a.masked_fill_(torch.eye(n, C).byte(), 0.0)
#         b = (a * torch.log(a)).sum(dim=1) # already log probs
#         c = (torch.matmul(a, mean_log_probs_n_C.t()) * torch.eye(n, n)).sum(dim=1) # n, C
#         d = torch.matmul(a, mean_log_probs_n_C.t())
#         d = d.masked_fill_(torch.eye(n, n).byte(), 0.0).sum(dim=1) # n, C

# #         print("a.shape:", a.shape)
# #         print("b.shape:", b.shape)
# #         print("c.shape:", c.shape)
# #         print("d.shape:", d.shape)
#         nats_n = b - c - d #.sum(dim=1)

#         pbar.update(end - start)

#     pbar.close()

#     return entropies_N
#####
def compute_entropy_vec(log_probs_N_K_C: torch.Tensor, device) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    entropies_N = torch.empty(N, dtype=torch.double)

    pbar = tqdm(total=N, desc="Entropy", leave=False)

    @toma.execute.chunked(log_probs_N_K_C, 1024)
    def compute(log_probs_n_K_C, start: int, end: int):
        mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
#         nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)
        n = log_probs_n_K_C.shape[0]
#         print("1:", torch.exp(log_probs_n_K_C[:, :, :, None]).shape) # .transpose(2, 3)
#         print("2:", torch.exp(log_probs_n_K_C)[:, :, None, :].shape) # .transpose(1, 2) #.transpose(1, 3)
#         print("1:", torch.exp(log_probs_n_K_C[:, :, None, :]).shape)
#         print("2:", torch.exp(log_probs_n_K_C)[:, None, :, :].transpose(2, 3).shape)

        # mb to use expand func as in joint prob file
        a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, :, None].to(device), torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, None, :].to(device)) # .transpose(1, 3) 
#         print("a.is_cuda:", a.is_cuda)
        # .permute(2, 1, 0) # .permute(2, 1, 0)
        # .permute(2, 1, 0, 3) # .permute(3, 1, 2, 0)
#         a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 0, 1), torch.exp(log_probs_n_K_C).permute(2, 1, 0))
    
#         print("a.shape:", a.shape)
#         a = a.permute(3, 2, 1, 0) # before: C, K, n, n

#         print("a.shape:", a.shape)
        zero_diag_mask = (torch.ones(n) - torch.eye(n)).repeat(C, K, 1, 1) #.view(n, n, K, C) #.permute(3, 2, 1, 0)
#         print("torch.ones(n) - torch.eye(n):", (torch.ones(n) - torch.eye(n)).shape)
#         print("torch.ones(n, n) - torch.eye(n, n):", (torch.ones(n, n) - torch.eye(n, n)).shape)
#         zero_diag_mask = torch.ones(n, n, K, C).diag_embed(0, dim1=0, dim2=1)
#         print("zero_diag_mask.shape:", zero_diag_mask.shape)
#         print("zero_diag_mask:", zero_diag_mask)
        a = a * zero_diag_mask.to(device)
#         print("a:", a)
#         print("a.shape:", a.shape)
        a = a.sum(dim=(1, 2)) / K
#         print("a.shape:", a.shape)
#         print("a:", a)
        a = a.t()
    
#         a = ((a * zero_diag_mask).sum(dim=(1, 2)) / K).t() ###
    
#         print("a.shape:", a.shape)
#         a = a.masked_fill_(torch.eye(n, C).byte(), 0.0) # fix
#         log_a = torch.log(a)
#         log_a[log_a == '-inf'] = 0.0

#         b = (a * torch.log(1.0 + a)).sum(dim=1) # logsumexp mb
    
#         b = b*(n-1) #? # should be equivalent to n sums as in c and d

#         c = (torch.matmul(a, mean_log_probs_n_C.t()) * torch.eye(n, n)).sum(dim=1) # n, C ###
#         c = c*(n-1) # because on each n-1 jth will be n-1 ith ###
#         d = torch.matmul(a, mean_log_probs_n_C.t()) ###
        
#         print("d:", d)
#         print("d.masked_fill_(torch.eye(n, n).bool(), 0.0):", d.masked_fill_(torch.eye(n, n).bool(), 0.0))

#         d = d.masked_fill_(torch.eye(n, n).bool(), 0.0).sum(dim=1) # n, C ###

#         print("a.shape:", a.shape)
#         print("b.shape:", b.shape)
#         print("c.shape:", c.shape)
#         print("d.shape:", d.shape)

#         nats_n = b - c - d
    
        c = torch.matmul(torch.exp(mean_log_probs_n_C).permute(1, 0)[:, :, None].to(device), torch.exp(mean_log_probs_n_C).permute(1, 0)[:, None, :].to(device)) #was without exp
        zero_diag_mask2 = (torch.ones(n) - torch.eye(n)).repeat(C, 1, 1)
#         print("zero_diag_mask2.shape:", zero_diag_mask2.shape)
#         print("zero_diag_mask2:", zero_diag_mask2)
#         print("c.is_cuda:", c.is_cuda)
        c = c * zero_diag_mask2.to(device)
        c = c.sum(dim=1)
        
#         print("c.shape:", c.shape)
        c = c.t()
    
#         с = (c * zero_diag_mask2).sum(dim=1).t() ###
    
#         c = (a * c).sum(dim=1)
#         print("c.shape:", c.shape)
#         nats_n = b - c
    
        nats_n = (a * (torch.log(1.0 + a) - torch.log(1.0 + c))).sum(dim=1) # was just c
    
#         print("nats_n:", nats_n)
#         print("a:", a)
#         print("torch.log(1.0 + a):", torch.log(1.0 + a))
#         print("b:", b)
#         print("c:", c)
#         print("torch.log(1.0 + c):", torch.log(1.0 + c))
        
#         print("nats_n:", nats_n)
#         print("d:", d)
#         print("end-start:", end-start)
#         print("nats_n.shape:", nats_n.shape)
        entropies_N[start:end].copy_(-nats_n) # sign

        pbar.update(end - start)

    pbar.close()

    return entropies_N
#####
# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
#     N, K, C = log_probs_N_K_C.shape

#     entropies_N = torch.empty(N, dtype=torch.double)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
# #         nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)
#         n = log_probs_n_K_C.shape[0]
# #         print("1:", torch.exp(log_probs_n_K_C[:, :, :, None]).shape) # .transpose(2, 3)
# #         print("2:", torch.exp(log_probs_n_K_C)[:, :, None, :].shape) # .transpose(1, 2) #.transpose(1, 3)
# #         print("1:", torch.exp(log_probs_n_K_C[:, :, None, :]).shape)
# #         print("2:", torch.exp(log_probs_n_K_C)[:, None, :, :].transpose(2, 3).shape)

#         # mb to use expand func as in joint prob file
# #         a = torch.matmul(torch.exp(log_probs_n_K_C[:, :, :, None]), torch.exp(log_probs_n_K_C)[:, :, None, :]) # .transpose(1, 3)
#         a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 0, 1), torch.exp(log_probs_n_K_C).permute(2, 1, 0))
    
# #         print("a.shape:", a.shape)
#         a = a.permute(2, 1, 0).sum(dim=2)
# #         a = a.sum(dim=(1, 2)) / K
# #         print("a.shape:", a.shape)
#         a = a.masked_fill_(torch.eye(n, n).byte(), 0.0) # n, C
#         b = (a * torch.log(a)).sum(dim=1)
#         c = (torch.matmul(a, mean_log_probs_n_C.t()) * torch.eye(n, n)).sum(dim=1) # n, C
#         d = torch.matmul(a, mean_log_probs_n_C.t())
#         d = d.masked_fill_(torch.eye(n, n).byte(), 0.0).sum(dim=1) # n, C

# #         print("a.shape:", a.shape)
# #         print("b.shape:", b.shape)
# #         print("c.shape:", c.shape)
# #         print("d.shape:", d.shape)
#         nats_n = b - c - d #.sum(dim=1)

#         pbar.update(end - start)

#     pbar.close()

#     return entropies_N

def get_lbb_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    scores_N += compute_entropy(log_probs_N_K_C)

    scores_N -= compute_entropy_vec(log_probs_N_K_C, device)
#     print("scores_N:", scores_N)

    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

#########################

def get_random_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape
    
    batch_size = min(batch_size, N)
    
    candidate_indices = []
    candidate_scores = []

    candiate_scores, candidate_indices = torch.rand(batch_size), torch.multinomial(torch.range(0, N, 1), num_samples=batch_size, replacement=False)

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

#########################
# ens setting #

# def init_glorot(model):
#     for module in model.modules():
#         if isinstance(module, (nn.Linear, nn.Conv2d)):
#             nn.init.xavier_uniform_(module.weight)
            
# def ens(model, T):
#     models = []
#     for _ in range(T):
#         model = make_model() #re-init model
#         model.apply(init_glorot)
#         _, model = train()
#         models.append(model)
    
#     otput = []
#     for model in models:
#         output.append(torch.softmax(model(batch), dim=-1))
#     output = torch.stack(output, dim=0)
#     output = output.mean(dim=0)
    
#     return output

def get_powerlbb_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None, alpha=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    scores_N += compute_entropy(log_probs_N_K_C)

    scores_N -= compute_entropy_vec(log_probs_N_K_C, device)
    scores_N[scores_N < 0] = 0.0
#     print("scores_N:", scores_N)
#     print("scores_N min orig:", torch.min(scores_N))
    
    scores_N = torch.pow(scores_N, alpha) # 5
#     print("scores_N:", scores_N)
#     print("min:", torch.min(scores_N))
#     print("sum scores_N:", torch.sum(scores_N))
    scores_N /= torch.sum(scores_N)
#     print("dist:", scores_N)
#     print("max:", torch.max(scores_N))
#     print("min:", torch.min(scores_N))

    candidate_indices = torch.multinomial(scores_N, batch_size, replacement=False)
#     print("candidate_indices:", candidate_indices)
    candiate_scores = scores_N[candidate_indices]
#     print("candiate_scores:", candiate_scores)

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

def get_powerbald_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None, alpha=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    scores_N += compute_entropy(log_probs_N_K_C)
#     print("probs_N_K_C:", torch.exp(log_probs_N_K_C))
#     print("compute_conditional_entropy(log_probs_N_K_C):", compute_conditional_entropy(log_probs_N_K_C))
#     print("compute_entropy(log_probs_N_K_C):", compute_entropy(log_probs_N_K_C))
    
    scores_N = torch.pow(scores_N, alpha)
    scores_N /= torch.sum(scores_N)
    candidate_indices = torch.multinomial(scores_N, batch_size, replacement=False)
    candiate_scores = scores_N[candidate_indices]

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

# def get_dpp_lbb_batch2(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None, alpha=None) -> CandidateBatch:
#     N, K, C = log_probs_N_K_C.shape

#     batch_size = min(batch_size, N)

#     candidate_indices = []
#     candidate_scores = []

#     scores_N = -compute_conditional_entropy(log_probs_N_K_C)
#     scores_N += compute_entropy(log_probs_N_K_C)

#     entropies_N = torch.empty(N, dtype=torch.double)
#     nats_nn = torch.empty((N, N), dtype=torch.double) # mb fill it with zeros and then plus small values (to prevent zeros); sampling is proport to mut info b/w (bigger val-s are more interesting)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
#         n = log_probs_n_K_C.shape[0]
#         a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, :, None].to(device), torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, None, :].to(device))

#         zero_diag_mask = (torch.ones(n) - torch.eye(n)).repeat(C, K, 1, 1) # [C, K, n, n]
#         a = a * zero_diag_mask.to(device)
#         a_nn = a.sum(dim=1) / K
#         a_nn = a_nn.view(n, n, C) # [n, n, C]
#         a = a.sum(dim=(1, 2)) / K
#         a = a.t()
    
#         c = torch.matmul(torch.exp(mean_log_probs_n_C).permute(1, 0)[:, :, None].to(device), torch.exp(mean_log_probs_n_C).permute(1, 0)[:, None, :].to(device)) 
#         zero_diag_mask2 = (torch.ones(n) - torch.eye(n)).repeat(C, 1, 1) # [C, n, n]
#         c = c * zero_diag_mask2.to(device)
#         c_nn = c.view(n, n, C) # [n, n, C]
#         c = c.sum(dim=1)
#         c = c.t()
    
#         nats_n = (a * (torch.log(1.0 + a) - torch.log(1.0 + c))).sum(dim=1) # [n]
#         temp_nn = (a_nn * (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn))).sum(dim=2) # [n, n]

#         entropies_N[start:end].copy_(-nats_n)
# #         print("nats_nn[start:end][start:end].shape:", nats_nn[start:end][start:end].shape) 
# #         print("a_nn.shape:", a_nn.shape)
# #         print("c_nn.shape:", c_nn.shape)
# #         for i in range(start, end+1):
# #         print("start:", start)
# #         print("end:", end)
# #         print("nats_nn.shape", nats_nn.shape)
# #         print("nats_nn[start:end, start:end].shape:", nats_nn[start:end, start:end].shape)
# #         print("temp_nn.shape:", temp_nn.shape)
#         nats_nn[start:end, start:end].copy_(temp_nn) # 44K vs 1024 # check on minus
# #         print("nats_nn[start:end, start:end]:", nats_nn[start:end, start:end])
# #         print("nats_nn.shape:", nats_nn.shape)
# #         nats_nn[start:end][start:end].copy_((a_nn * (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn))).sum(dim=2))
#         pbar.update(end - start)

#     pbar.close()
# #     print("nats_nn:", nats_nn)
#     scores_N -= entropies_N
#     scores_N[scores_N <= 0] = 1e-12 # to prevent <= zeros
# #     print("scores_N:", scores_N)

#     candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)
# #     print("candiate_scores:", candiate_scores)
# #     print("candidate_indices:", candidate_indices)
    
# #     micro = 1e-12
# #     nats_nn += np.random.random(nats_nn.shape) * micro # mb remove
# #     print("nats_nn:", nats_nn) # too big, probably

# #     nats_nn[nats_nn <=0] = 1e-12

#     nats_ind_nn = nats_nn.index_select(0, candidate_indices).view(len(candidate_indices), N)
    
# #     print("nats_ind_nn.shape:", nats_ind_nn.shape)

#     nats_ind_nn = nats_ind_nn.index_select(1, candidate_indices).view(len(candidate_indices), len(candidate_indices))
    
#     # [candidate_indices, candidate_indices].view(len(candidate_indices), len(candidate_indices)) # need to take not only diag but all combs
    
# #     print("nats_ind_nn.shape:", nats_ind_nn.shape)
# #     print("nats_ind_nn:", nats_ind_nn) # not symm # ? neg scores are ok?
# #     print("nats_ind_nn.T:", nats_ind_nn.T)
# #     print("torch.all(torch.eq(nats_ind_nn, nats_ind_nn.T)):", torch.all(torch.eq(nats_ind_nn, nats_ind_nn.T)))
# #     print("torch.where((nats_ind_nn != nats_ind_nn.T).all(dim=0))[0]:", torch.where((nats_ind_nn != nats_ind_nn.T).all(dim=0))[0])

# #     array = is_square(nats_ind_nn)
# #     print("array:", array)

# #     idx = np.arange(min(20, nats_ind_nn.shape[0]))
# #     print("idx:", idx)
# #     M = nats_ind_nn[np.ix_(idx, idx)]
# #     print("M.shape:", M.shape)
# #     print("M:", M)
# #     print("M.T.shape:", M.T.shape)
# #     print("M.T:", M.T)
    
# #     if np.allclose(M.T, M):
# #         print("True")
#     nats_ind_nn = torch.eye(nats_ind_nn.shape[0]) * torch.diag(nats_ind_nn)
#     nats_nn[nats_nn < 0] = 0.0
#     dpp = FiniteDPP('likelihood', **{'L': nats_ind_nn}) # eig vals should be >= 0 -- for now not
# #     print("torch.count_zero(nats_ind_nn):", nats_nn.shape[0] * nats_nn.shape[1] - torch.count_nonzero(nats_nn))
# #     dpp = FiniteDPP('likelihood', **{'L': nats_nn[candidate_indices, candidate_indices]})
#     ATTEMPTS = 5 # how many times do we sample
#     for _ in range(ATTEMPTS):
#         dpp.sample_exact()
#         ids = dpp.list_of_samples[-1]
#         if len(ids):  # We should retry if mask is zero-length
#             break
    
#     candiate_scores, candidate_indices = candiate_scores[ids], candidate_indices[ids]

#     return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

def _rank(eigen_values):
    rank = np.count_nonzero(eigen_values > 1e-3)
    return rank

def get_nu(eigen_values, k):
    """
    Get tilting coefficient to correctly approximate k-dpp marginal probabilities
    See amblard2018
    :param eigen_values: eigen values of L (likelihood) matrix for dpp
    :param k: how much samples do we plan to take
    :return: tilting coefficient
    """
    values = eigen_values + 1e-14
    def point(nu):
        exp_nu = np.exp(nu)
        expect = np.sum([val*exp_nu / (1 + exp_nu*val) for val in values])
        return expect - k # should be zero if nu is a solution

    try:
        solution = root_scalar(point, bracket=[-10., 10.]) # search for nu coeff in [-10., 10.]
        assert solution.converged
        return solution.root
    except (ValueError, AssertionError):
        raise ValueError('k-dpp: Probably too small matrix rank for the k')

def get_dpp_lbb_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None, alpha=5, power=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    scores_N += compute_entropy(log_probs_N_K_C)
    
    if power:
        scores_N = torch.pow(scores_N, alpha)
        scores_N /= torch.sum(scores_N)
        candidate_indices = torch.multinomial(scores_N, batch_size, replacement=False)
        candiate_scores = scores_N[candidate_indices]
    else:
        candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)

    log_probs_n_K_C = log_probs_N_K_C.index_select(0, candidate_indices) # ex.: [100, 5, 10]
#     print("log_probs_n_K_C.shape:", log_probs_n_K_C.shape)

    mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
    n = log_probs_n_K_C.shape[0]
    a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, :, None].to(device), torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, None, :].to(device))

    zero_diag_mask = (torch.ones(n) - torch.eye(n)).repeat(C, K, 1, 1) # [C, K, n, n]
    a = a * zero_diag_mask.to(device)
    a_nn = a.sum(dim=1) / K # [C, n, n]
#     a_nn = a_nn.view(n, n, C) # [n, n, C]
    
#     print("torch.all(torch.eq(a_nn, a_nn.T)):", torch.all(torch.eq(a_nn.view(a_nn.shape[0], a_nn.shape[1], a_nn.shape[2]), a_nn.view(a_nn.shape[1], a_nn.shape[0], a_nn.shape[2]))))

    c = torch.matmul(torch.exp(mean_log_probs_n_C).permute(1, 0)[:, :, None].to(device), torch.exp(mean_log_probs_n_C).permute(1, 0)[:, None, :].to(device))
#     print("c.shape:", c.shape)
#     print("torch.all(torch.eq(c, c.T)):", torch.all(torch.eq(c.view(c.shape[0], c.shape[1], c.shape[2]), c.view(c.shape[0], c.shape[2], c.shape[1]))))
    zero_diag_mask2 = (torch.ones(n) - torch.eye(n)).repeat(C, 1, 1) # [C, n, n]
#     print("torch.all(torch.eq(zero_diag_mask2, zero_diag_mask2.T)):", torch.all(torch.eq(zero_diag_mask2.view(zero_diag_mask2.shape[0], zero_diag_mask2.shape[1], zero_diag_mask2.shape[2]), zero_diag_mask2.view(zero_diag_mask2.shape[0], zero_diag_mask2.shape[2], zero_diag_mask2.shape[1]))))
    c_nn = c * zero_diag_mask2.to(device) # [C, n, n]
#     c_nn = c_nn.view(n, n, C) # [n, n, C]

#     print("torch.all(torch.eq(c_nn, c_nn.T)):", torch.all(torch.eq(c_nn.view(c_nn.shape[0], c_nn.shape[1], c_nn.shape[2]), c_nn.view(c_nn.shape[1], c_nn.shape[0], c_nn.shape[2]))))

    nats_nn = (a_nn * (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn))).sum(dim=0) # [n, n]
#     print("nats_nn:", nats_nn)
#     print("torch.all(torch.eq(nats_nn, nats_nn.T)):", torch.all(torch.eq(nats_nn, nats_nn.T)))
#     b = (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn))
#     d = (a_nn * (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn)))
#     print("d.shape:", d.shape)
#     print("torch.all(torch.eq(b, b.T)):", torch.all(torch.eq(b.view(b.shape[0], b.shape[1], b.shape[2]), b.view(b.shape[1], b.shape[0], b.shape[2]))))
#     print("torch.all(torch.eq(d, d.T)):", torch.all(torch.eq(d.view(d.shape[0], d.shape[1], d.shape[2]), d.view(d.shape[1], d.shape[0], d.shape[2]))))
#     nats_nn[nats_nn <= 0] = 10e-12
#     print("d[:, :, 0]:", d[:, :, 0].view(d.shape[0], d.shape[1], 1))
#     print("d[:, :, 0]:", d[:, :, 0].view(d.shape[1], d.shape[0], 1))

#     nats_nn = torch.eye(nats_nn.shape[0]).to(device) * torch.diag(nats_nn).to(device)
    nats_nn[nats_nn <= 0] = 1e-12 # 10e-6 -- best # last was 1e-12
#     print("torch.max(nats_nn):", torch.max(nats_nn))
#     print("torch.min(nats_nn):", torch.min(nats_nn))
    L = nats_nn.detach().cpu().numpy()
#     savez_compressed('L_before_noise.npz', L)
# mb norm option
#     dpp = FiniteDPP('likelihood', **{'L': L.detach().cpu().numpy()})
#     ATTEMPTS = 5 # how many times do we try to sample
#     for _ in range(ATTEMPTS):
#         dpp.sample_exact()
#         ids = dpp.list_of_samples[-1]
#         if len(ids):  # We should retry if mask is zero-length
#             break
    noise_level = 0.5 # 0.2 # 1e-3 
    L += noise_level * np.eye(len(L))
    eigen_values = np.linalg.eigh(L)[0]
#     print("eigen_values:", eigen_values)
#     print("eigen_values.max():", eigen_values.max())
#     print("eigen_values.min():", eigen_values.min())
#     print("rank L:", _rank(eigen_values))
    k = int(batch_size/10)
    nu = get_nu(eigen_values, k)
    I = torch.eye(len(L)).to(device)
    L_tilted = np.exp(nu) * torch.DoubleTensor(L).to(device)
    K_tilted = torch.mm(L_tilted, torch.inverse(L_tilted + I)).double()
#     kdpp = FiniteDPP('correlation', **{"K": K_tilted.detach().cpu().numpy()})
    kdpp = FiniteDPP('likelihood', **{"L": L_tilted.detach().cpu().numpy()})
    ids = kdpp.sample_exact_k_dpp(k)
    
    candiate_scores, candidate_indices = candiate_scores[ids], candidate_indices[ids]
    
#     savez_compressed('eigen_values.npz', eigen_values)
#     savez_compressed('log_eigen_values.npz', np.log(eigen_values))
#     savez_compressed('L.npz', L)
#     savez_compressed('L_tilted.npz', L_tilted.detach().cpu().numpy())

    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())

###################
# def get_dpp_lbb_batch(log_probs_n_K_C: torch.Tensor, batch_size: int, dtype=None, device=None, alpha=None) -> CandidateBatch:
#     n, K, C = log_probs_n_K_C.shape

#     batch_size = min(batch_size, n)

#     candidate_indices = []
#     candidate_scores = []

#     scores_N = -compute_conditional_entropy(log_probs_n_K_C)
#     scores_N += compute_entropy(log_probs_n_K_C)
    
#     entropies_N = torch.empty(n, dtype=torch.double)
#     nats_nn = torch.empty((n, n), dtype=torch.double)

#     pbar = tqdm(total=n, desc="Entropy", leave=False)

#     mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
#     n = log_probs_n_K_C.shape[0]
#     a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, :, None].to(device), torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, None, :].to(device))

#     zero_diag_mask = (torch.ones(n) - torch.eye(n)).repeat(C, K, 1, 1) # [C, K, n, n]
#     a = a * zero_diag_mask.to(device)
#     a_nn = a.sum(dim=1) / K
#     a_nn = a_nn.view(n, n, C) # [n, n, C]
#     a = a.sum(dim=(1, 2)) / K
#     a = a.t()

#     c = torch.matmul(torch.exp(mean_log_probs_n_C).permute(1, 0)[:, :, None].to(device), torch.exp(mean_log_probs_n_C).permute(1, 0)[:, None, :].to(device)) 
#     zero_diag_mask2 = (torch.ones(n) - torch.eye(n)).repeat(C, 1, 1) # [C, n, n]
#     c = c * zero_diag_mask2.to(device)
#     c_nn = c.view(n, n, C) # [n, n, C]
#     c = c.sum(dim=1)
#     c = c.t()

#     nats_n = (a * (torch.log(1.0 + a) - torch.log(1.0 + c))).sum(dim=1) # [n]
# #         nats_nn = (a_nn * (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn))).sum(dim=2) # [n, n]

#     entropies_N.copy_(-nats_n)
#     nats_nn.copy_((a_nn * (torch.log(1.0 + a_nn) - torch.log(1.0 + c_nn))).sum(dim=2))
#     pbar.update(end - start)

#     pbar.close()
#     print("nats_nn:", nats_nn)
#     scores_N -= entropies_N
#     scores_N[scores_N < 0] = 1e-12 # to prevent zeros
# #     print("scores_N:", scores_N)

#     candiate_scores, candidate_indices = torch.topk(scores_N, batch_size*10)
    
#     micro = 1e-12
#     nats_nn += np.random.random(nats_nn.shape) * micro # mb remove
    
#     dpp = FiniteDPP('likelihood', **{'L': nats_nn[candidate_indices][candidate_indices]})
    
#     ATTEMPTS = 5 # mb fix
#     for _ in range(ATTEMPTS):
#         dpp.sample_exact()
#         ids = dpp.list_of_samples[-1]
#         if len(ids):  # We should retry if mask is zero-length
#             break
    
#     candiate_scores, candidate_indices = candiate_scores[ids], candidate_indices[ids]

#     return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())
##################################
# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor, device) -> torch.Tensor:
#     N, K, C = log_probs_N_K_C.shape

#     entropies_N = torch.empty(N, dtype=torch.double)

#     pbar = tqdm(total=N, desc="Entropy", leave=False)

#     @toma.execute.chunked(log_probs_N_K_C, 1024)
#     def compute(log_probs_n_K_C, start: int, end: int):
#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
#         n = log_probs_n_K_C.shape[0]

#         a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, :, None].to(device), torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, None, :].to(device))
#         zero_diag_mask = (torch.ones(n) - torch.eye(n)).repeat(C, K, 1, 1)
#         a = a * zero_diag_mask.to(device)
#         a = a.sum(dim=(1, 2)) / K
#         a = a.t()
    
#         c = torch.matmul(torch.exp(mean_log_probs_n_C).permute(1, 0)[:, :, None].to(device), torch.exp(mean_log_probs_n_C).permute(1, 0)[:, None, :].to(device))
#         zero_diag_mask2 = (torch.ones(n) - torch.eye(n)).repeat(C, 1, 1)
#         c = c * zero_diag_mask2.to(device)
#         c = c.sum(dim=1)
#         c = c.t()

#         nats_n = (a * (torch.log(1.0 + a) - torch.log(1.0 + c))).sum(dim=1)
#         entropies_N[start:end].copy_(-nats_n)

#         pbar.update(end - start)

#     pbar.close()

#     return entropies_N