{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp batchbald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither src found as subdirectory in %s nor was a notebooks directory found!\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchBALD Algorithm\n",
    "> Greedy algorithm and score computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement two helper classes to compute conditional entropies $H[y_i|w]$ and entropies $H[y_i]$. \n",
    "Then, we will implement BatchBALD and BALD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from toma import toma\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from batchbald_redux import joint_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define a couple of sampled distributions to use for our testing our code.\n",
    "\n",
    "$K=20$ means 20 inference samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-f9e180f29659>:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.stack(list(map(torch.as_tensor, l)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mixture_prob_dist(p1, p2, m):\n",
    "    return (1.0 - m) * np.asarray(p1) + m * np.asarray(p2)\n",
    "\n",
    "\n",
    "p1 = [0.7, 0.1, 0.1, 0.1]\n",
    "p2 = [0.3, 0.3, 0.2, 0.2]\n",
    "y1_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.7, 0.1, 0.1]\n",
    "p2 = [0.2, 0.3, 0.3, 0.2]\n",
    "y2_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.1, 0.7, 0.1]\n",
    "p2 = [0.2, 0.2, 0.3, 0.3]\n",
    "y3_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.1, 0.1, 0.7]\n",
    "p2 = [0.3, 0.2, 0.2, 0.3]\n",
    "y4_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "\n",
    "def nested_to_tensor(l):\n",
    "    return torch.stack(list(map(torch.as_tensor, l)))\n",
    "\n",
    "\n",
    "ys_ws = nested_to_tensor([y1_ws, y2_ws, y3_ws, y4_ws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 classes probs (sum = 1) x 20 inference samples (dropouts, models)\n",
    "# y1_ws -- just one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "p = [0.25, 0.25, 0.25, 0.25]\n",
    "yu_ws = [p for m in range(K)]\n",
    "yus_ws = nested_to_tensor([yu_ws] * 4) # 0.25 is everywhere in that shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_ws.shape # 4 objs x 20 inference samples x 4 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Entropies and Batched Entropies\n",
    "\n",
    "To start with, we write two functions to compute the conditional entropy $H[y_i|w]$ and the entropy $H[y_i]$ for each input sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_entropy(probs_N_K_C: torch.Tensor) -> torch.Tensor: # help to check at the end\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double) # good thing to immediately use data type\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Conditional Entropy\", leave=False) # progress bar\n",
    "#     print(\"probs_N_K_C:\", probs_N_K_C)\n",
    "\n",
    "    @toma.execute.chunked(probs_N_K_C, 1024) # to allocate GPU memory starting from batch=1024 and decresing if needed\n",
    "    def compute(probs_n_K_C, start: int, end: int):\n",
    "#     ^ above is an interface of toma: def compute(chunk, start, end):\n",
    "#     N (number of objs) can be really big -- make a chunk of N: n\n",
    "\n",
    "    # (chunk) chunk result and pass the chunks to compute_result one by one\n",
    "    # range iterates over range(start, end, step)\n",
    "    # execute -- just to exec func\n",
    "    \n",
    "    # probs_n_K_C, start, end -- depending on batch size will select\n",
    "    # for 4*20*4 = 320 will take all at once\n",
    "\n",
    "#         print(\"probs_n_K_C:\", probs_n_K_C) # is the same as probs_N_K_C but not defined\n",
    "#         print(\"start:\", start) # 0\n",
    "#         print(\"end:\", end) # 4\n",
    "        nats_n_K_C = probs_n_K_C * torch.log(probs_n_K_C)\n",
    "        # to remove infs from log of zeros\n",
    "        nats_n_K_C[probs_n_K_C == 0] = 0.0\n",
    "\n",
    "        # summed for all inference samp-s (simultaneosly) and classes and average over samples at the end\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_K_C, dim=(1, 2)) / K) # write chunk by chunk # return a copy\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N\n",
    "\n",
    "\n",
    "def compute_entropy(probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Entropy\", leave=False)\n",
    "\n",
    "    @toma.execute.chunked(probs_N_K_C, 1024)\n",
    "    def compute(probs_n_K_C, start: int, end: int):\n",
    "        mean_probs_n_C = probs_n_K_C.mean(dim=1) # from start operate with mean by inference samples probs\n",
    "        nats_n_C = mean_probs_n_C * torch.log(mean_probs_n_C) # entr of mean probs\n",
    "        nats_n_C[mean_probs_n_C == 0] = 0.0\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1)) # at the end sum by classes\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure everything is computed correctly.\n",
    "\n",
    "assert np.allclose(compute_conditional_entropy(yus_ws), [1.3863, 1.3863, 1.3863, 1.3863], atol=0.1)\n",
    "assert np.allclose(compute_entropy(yus_ws), [1.3863, 1.3863, 1.3863, 1.3863], atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our neural networks usually use a `log_softmax` as final layer. To avoid having to call `.exp_()`, which is easy to miss and annoying to debug, we will instead use a version that uses `log_probs` instead of `probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def compute_conditional_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Conditional Entropy\", leave=False)\n",
    "\n",
    "    @toma.execute.chunked(log_probs_N_K_C, 1024)\n",
    "    def compute(log_probs_n_K_C, start: int, end: int):\n",
    "        nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_K_C, dim=(1, 2)) / K) # mean by samps at the end\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N\n",
    "\n",
    "\n",
    "def compute_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Entropy\", leave=False)\n",
    "\n",
    "    @toma.execute.chunked(log_probs_N_K_C, 1024)\n",
    "    def compute(log_probs_n_K_C, start: int, end: int):\n",
    "        mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K) # mean of log probs\n",
    "        nats_n_C = mean_log_probs_n_C * torch.exp(mean_log_probs_n_C)\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1)) # sum by classes at the end\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# Make sure everything is computed correctly.\n",
    "assert np.allclose(compute_conditional_entropy(yus_ws.log()), [1.3863, 1.3863, 1.3863, 1.3863], atol=0.1)\n",
    "assert np.allclose(compute_entropy(yus_ws.log()), [1.3863, 1.3863, 1.3863, 1.3863], atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2069, 1.2069, 1.2069, 1.2069], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "conditional_entropies = compute_conditional_entropy(ys_ws.log())\n",
    "\n",
    "print(conditional_entropies)\n",
    "\n",
    "assert np.allclose(conditional_entropies, [1.2069, 1.2069, 1.2069, 1.2069], atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2376, 1.2376, 1.2376, 1.2376], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "entropies = compute_entropy(ys_ws.log())\n",
    "\n",
    "print(entropies)\n",
    "\n",
    "assert np.allclose(entropies, [1.2376, 1.2376, 1.2376, 1.2376], atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchBALD\n",
    "\n",
    "To compute BatchBALD exactly for a candidate batch, we'd have to compute $I[(y_b)_B;w] = H[(y_b)_B] - H[(y_b)_B|w]$.\n",
    "\n",
    "As the $y_b$ are independent given $w$, we can simplify $H[(y_b)_B|w] = \\sum_b H[y_b|w]$.\n",
    "\n",
    "Furthermore, we use a greedy algorithm to build up the candidate batch, so $y_1,\\dots,y_{B-1}$ will stay fixed as we determine $y_{B}$. We compute\n",
    "$H[(y_b)_{B-1}, y_i] - H[y_i|w]$ for each pool element $y_i$ and add the highest scorer as $y_{B}$.\n",
    "\n",
    "We don't utilize the last optimization here in order to compute the actual scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the Paper\n",
    "\n",
    "![BatchBALD algorithm in the paper](batchbald_algorithm.png)\n",
    "\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass # make it easier to create dataclasses\n",
    "class CandidateBatch:\n",
    "    scores: List[float]\n",
    "    indices: List[int]\n",
    "\n",
    "\n",
    "def get_batchbald_batch(\n",
    "    log_probs_N_K_C: torch.Tensor, batch_size: int, num_samples: int, dtype=None, device=None\n",
    ") -> CandidateBatch:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    batch_size = min(batch_size, N)\n",
    "\n",
    "    candidate_indices = []\n",
    "    candidate_scores = []\n",
    "\n",
    "    if batch_size == 0:\n",
    "        return CandidateBatch(candidate_scores, candidate_indices) # returns class with scores and indeces\n",
    "\n",
    "    conditional_entropies_N = compute_conditional_entropy(log_probs_N_K_C)\n",
    "    # all above ?\n",
    "    batch_joint_entropy = joint_entropy.DynamicJointEntropy(\n",
    "        num_samples, batch_size - 1, K, C, dtype=dtype, device=device\n",
    "    )\n",
    "\n",
    "    # We always keep these on the CPU. # why cuda_is_available in this case?\n",
    "    scores_N = torch.empty(N, dtype=torch.double, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    for i in tqdm(range(batch_size), desc=\"BatchBALD\", leave=False):\n",
    "        if i > 0:\n",
    "            latest_index = candidate_indices[-1]\n",
    "            batch_joint_entropy.add_variables(log_probs_N_K_C[latest_index : latest_index + 1])\n",
    "\n",
    "        shared_conditinal_entropies = conditional_entropies_N[candidate_indices].sum()\n",
    "\n",
    "        batch_joint_entropy.compute_batch(log_probs_N_K_C, output_entropies_B=scores_N)\n",
    "\n",
    "        scores_N -= conditional_entropies_N + shared_conditinal_entropies\n",
    "        scores_N[candidate_indices] = -float(\"inf\")\n",
    "\n",
    "        candidate_score, candidate_index = scores_N.max(dim=0)\n",
    "\n",
    "        candidate_indices.append(candidate_index.item())\n",
    "        candidate_scores.append(candidate_score.item())\n",
    "\n",
    "    return CandidateBatch(candidate_scores, candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BatchBALD:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExactJointEntropy.compute_batch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExactJointEntropy.compute_batch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExactJointEntropy.compute_batch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExactJointEntropy.compute_batch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CandidateBatch(scores=[0.030715639666234917, 0.05961958627158248, 0.0869107051474467, 0.11275304532467878], indices=[1, 0, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batchbald_batch(ys_ws.log().double(), 4, 1000, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BALD\n",
    "\n",
    "BALD is the same as BatchBALD, except that we evaluate points individually, by computing $I[y_i;w]$ for each, and then take the top $B$ scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def get_bald_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    batch_size = min(batch_size, N)\n",
    "\n",
    "    candidate_indices = []\n",
    "    candidate_scores = []\n",
    "\n",
    "    scores_N = -compute_conditional_entropy(log_probs_N_K_C)\n",
    "    scores_N += compute_entropy(log_probs_N_K_C)\n",
    "\n",
    "    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)\n",
    "\n",
    "    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CandidateBatch(scores=[0.030715639666234917, 0.030715639666234917, 0.030715639666234917, 0.030715639666234695], indices=[1, 2, 0, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bald_batch(ys_ws.log().double(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Batch BALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # way #1\n",
    "\n",
    "# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "#     N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "#     entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "#     pbar = tqdm(total=N, desc=\"Entropy\", leave=False)\n",
    "\n",
    "#     @toma.execute.chunked(log_probs_N_K_C, 1024)\n",
    "#     def compute(log_probs_n_K_C, start: int, end: int):\n",
    "# #         print(\"log_probs_n_K_C.shape:\", log_probs_n_K_C.shape)\n",
    "#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)\n",
    "# #         nats_n_C = torch.zeros(N, C) #?\n",
    "#         n = log_probs_n_K_C.shape[0]\n",
    "#         nats_n = torch.zeros(n) # N\n",
    "\n",
    "# #         for i in range(N-1):\n",
    "#         for i in range(n):\n",
    "# #             for j in range(i+1, N):\n",
    "#             for j in range(n): # N can be bigger than batch_size -> error # one of possible solut-s -- tensor view\n",
    "        \n",
    "# #                 print(\"i, j:\", (i, j))\n",
    "# #                 print(\"eq:\", torch.exp(mean_log_probs_n_C[i][:]) * torch.exp(mean_log_probs_n_C[j][:])*\\\n",
    "# #                 (mean_log_probs_n_C[i][:] + mean_log_probs_n_C[j][:]) - torch.exp(mean_log_probs_n_C[i][:])*\\\n",
    "# #                 mean_log_probs_n_C[i][:] - torch.exp(mean_log_probs_n_C[j][:]) * mean_log_probs_n_C[j][:])\n",
    "# #                 nats_n_C\n",
    "#                 if i != j:\n",
    "# #                     print(\"i, j:\", (i, j))\n",
    "# #                     print(\"mean_log_probs_n_C.shape:\", mean_log_probs_n_C.shape)\n",
    "#                     nats_n[i] += torch.sum(torch.exp(mean_log_probs_n_C[i][:]) * torch.exp(mean_log_probs_n_C[j][:])*\\\n",
    "#                     (mean_log_probs_n_C[i][:] + mean_log_probs_n_C[j][:]) - torch.exp(mean_log_probs_n_C[i][:])*\\\n",
    "#                     mean_log_probs_n_C[i][:] - torch.exp(mean_log_probs_n_C[j][:]) * mean_log_probs_n_C[j][:])\n",
    "# #                 print(\"nats_n_C:\", nats_n_C)\n",
    "\n",
    "# #                 print(\"nats_n:\", nats_n) ###\n",
    "    \n",
    "# #         print(\"nats_n_C.shape:\", nats_n_C.shape)\n",
    "# #         print(\"nats_n_C:\", nats_n_C)\n",
    "# #         print(\"mean_log_probs_n_C:\", mean_log_probs_n_C)\n",
    "#         entropies_N[start:end].copy_(nats_n)\n",
    "# #         -torch.sum(nats_n_C, dim=1)\n",
    "\n",
    "# #         print(\"entropies_N:\", entropies_N) ###\n",
    "    \n",
    "#         pbar.update(end - start)\n",
    "\n",
    "#     pbar.close()\n",
    "\n",
    "#     return entropies_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor way\n",
    "\n",
    "# def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "#     N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "#     entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "#     pbar = tqdm(total=N, desc=\"Entropy\", leave=False)\n",
    "\n",
    "#     @toma.execute.chunked(log_probs_N_K_C, 1024)\n",
    "#     def compute(log_probs_n_K_C, start: int, end: int):\n",
    "#         mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)\n",
    "#         n = log_probs_n_K_C.shape[0]\n",
    "\n",
    "#         a = torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones_like(mean_log_probs_n_C).fill_diagonal_(0.0), mean_log_probs_n_C.t()).fill_diagonal_(0.0)\n",
    "#         b = torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones_like(mean_log_probs_n_C).fill_diagonal_(0.0), mean_log_probs_n_C.t())*torch.eye(mean_log_probs_n_C.shape[0])\n",
    "#         c = torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t())*torch.eye(mean_log_probs_n_C.shape[0])*n\n",
    "#         d = torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t())*torch.ones_like(mean_log_probs_n_C).fill_diagonal_(0.0)\n",
    "        \n",
    "#         nats_n = (torch.matmul(a, b) - c - d).sum(dim=1)\n",
    "\n",
    "#         pbar.update(end - start)\n",
    "\n",
    "#     pbar.close()\n",
    "\n",
    "#     return entropies_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "\n",
    "# log_probs_n_K_C = copy.copy(ys_ws.log().double())\n",
    "# mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)\n",
    "# print(mean_log_probs_n_C.shape)\n",
    "\n",
    "# a = torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones_like(mean_log_probs_n_C).fill_diagonal_(0.0), mean_log_probs_n_C.t()).fill_diagonal_(0.0)\n",
    "# b = torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t())*torch.ones_like(mean_log_probs_n_C).fill_diagonal_(0.0), mean_log_probs_n_C.t())*torch.eye(mean_log_probs_n_C.shape[0])\n",
    "# c = torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t())*torch.eye(mean_log_probs_n_C.shape[0])*3 # how many times\n",
    "# d = torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t())*torch.ones_like(mean_log_probs_n_C).fill_diagonal_(0.0) # how many times\n",
    "\n",
    "# (torch.matmul(a, b) - c - d).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.matmul(torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t().fill_diagonal_(0.0)), mean_log_probs_n_C).fill_diagonal_(0.0) +\\\n",
    "# torch.matmul(torch.exp(mean_log_probs_n_C), torch.exp(mean_log_probs_n_C).t()).fill_diagonal_(0.0) -\\\n",
    "# torch.diag(torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t()))*n -\\\n",
    "# torch.matmul(torch.exp(mean_log_probs_n_C), mean_log_probs_n_C.t()).fill_diagonal_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_vec(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Entropy\", leave=False)\n",
    "\n",
    "    @toma.execute.chunked(log_probs_N_K_C, 1024)\n",
    "    def compute(log_probs_n_K_C, start: int, end: int):\n",
    "        mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)\n",
    "#         nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)\n",
    "        n = log_probs_n_K_C.shape[0]\n",
    "\n",
    "        # mb to use expand func as in joint prob file\n",
    "        a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, :, None], torch.exp(log_probs_n_K_C).permute(2, 1, 0)[:, :, None, :]) # .transpose(1, 3) \n",
    "        # .permute(2, 1, 0) # .permute(2, 1, 0)\n",
    "        # .permute(2, 1, 0, 3) # .permute(3, 1, 2, 0)\n",
    "#         a = torch.matmul(torch.exp(log_probs_n_K_C).permute(2, 0, 1), torch.exp(log_probs_n_K_C).permute(2, 1, 0))\n",
    "    \n",
    "        print(\"a.shape:\", a.shape)\n",
    "        a = a.permute(3, 2, 1, 0)\n",
    "        print(\"a.shape:\", a.shape)\n",
    "        a = a.sum(dim=(1, 2)) / K\n",
    "        print(\"a.shape:\", a.shape)\n",
    "        a = a.masked_fill_(torch.eye(n, C).byte(), 0.0)\n",
    "        b = (a * torch.log(a)).sum(dim=1)\n",
    "        c = (torch.matmul(a, mean_log_probs_n_C.t()) * torch.eye(n, n)).sum(dim=1) # n, C\n",
    "        d = torch.matmul(a, mean_log_probs_n_C.t())\n",
    "        d = d.masked_fill_(torch.eye(n, n).byte(), 0.0).sum(dim=1) # n, C\n",
    "\n",
    "#         print(\"a.shape:\", a.shape)\n",
    "#         print(\"b.shape:\", b.shape)\n",
    "#         print(\"c.shape:\", c.shape)\n",
    "#         print(\"d.shape:\", d.shape)\n",
    "        nats_n = b - c - d\n",
    "        entropies_N[start:end].copy_(nats_n)\n",
    "\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lbb_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    batch_size = min(batch_size, N)\n",
    "\n",
    "    candidate_indices = []\n",
    "    candidate_scores = []\n",
    "\n",
    "    scores_N = -compute_conditional_entropy(log_probs_N_K_C)\n",
    "    scores_N += compute_entropy(log_probs_N_K_C)\n",
    "\n",
    "    scores_N -= compute_entropy_vec(log_probs_N_K_C)\n",
    "\n",
    "    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)\n",
    "\n",
    "    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way #2\n",
    "\n",
    "# def get_lbb_batch(log_probs_N_K_C: torch.Tensor, batch_size: int, dtype=None, device=None) -> CandidateBatch:\n",
    "#     N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "#     batch_size = min(batch_size, N)\n",
    "\n",
    "#     candidate_indices = []\n",
    "#     candidate_scores = []\n",
    "\n",
    "#     scores_N = -compute_conditional_entropy(log_probs_N_K_C)\n",
    "#     scores_N += compute_entropy(log_probs_N_K_C)\n",
    "    \n",
    "#     log_probs_N_C = torch.sum(log_probs_N_K_C, dim=1)/K  # shold be according to log \n",
    "#     nats_N_C = torch.zeros(N, C)\n",
    "    \n",
    "#     for i in range(N-1):\n",
    "#         for j in range(i+1, N):\n",
    "#             nats_N_C += torch.exp(log_probs_N_C[i][:])*torch.exp(log_probs_N_C[j][:])*\\\n",
    "#             (log_probs_N_C[i][:] + log_probs_N_C[j][:]) - torch.exp(log_probs_N_C[i][:])*\\\n",
    "#             log_probs_N_C[i][:] - torch.exp(log_probs_N_C[j][:]) * log_probs_N_C[j][:]\n",
    "            \n",
    "#     scores_N += torch.sum(nats_N_C, dim=1)\n",
    "\n",
    "#     candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)\n",
    "\n",
    "#     return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CandidateBatch(scores=[1.2683128078987922, 1.2683128078987917, 0.030715639666234917, 0.030715639666234917], indices=[2, 3, 1, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lbb_batch(ys_ws.log().double(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.ones(3, 3)\n",
    "# torch.triu(a, diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mm(prob(y[i]), prob(y[j]))\n",
    "\n",
    "# making upper triang matr without diag\n",
    "# r,c = np.triu_indices(A.shape[0], 1)\n",
    "# A[r,c] = 0.0\n",
    "# torch.triu_indices()\n",
    "\n",
    "#     cond_entropy_ytheta = -torch.sum(log_probs_K_C * torch.exp(log_probs_K_C), dim=0)/K\n",
    "#     entropy = log_probs_C * torch.exp(log_probs_C)\n",
    "#     cond_entropy_yij = -torch.sum(log_probs_C_C * torch.exp(log_probs_C_C), dim=0)/C\n",
    "    # sum for all y_i, y_j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
