{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp joint_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither src found as subdirectory in %s nor was a notebooks directory found!\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Entropies\n",
    "> Computing joint entropies exactly or by using importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module helps compute joint entropies for dependent categorical variables given via a density $p((y_i)_i|w))$ in the Bayesian setting. We compute the density $p((y_i)_i)$ by marginalizing over $w$.\n",
    "\n",
    "Two cases are implemented:\n",
    "\n",
    "* exact joint entropies (which works for up 5 to joint variables depending on memory and # of classes);\n",
    "* estimated joint entropies using importance sampling of configurations.\n",
    "\n",
    "Note: \"exact\" based on the given draws of $w$. They are still an approximation because we do not integrate over $w$ but use Monte-Carlo samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of inference samples `K`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import torch\n",
    "from toma import toma\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run tests, we need a few sampled distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-a4f3ef56f743>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.stack(list(map(torch.as_tensor, l)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mixture_prob_dist(p1, p2, m):\n",
    "    return (1.0 - m) * np.asarray(p1) + m * np.asarray(p2)\n",
    "\n",
    "\n",
    "p1 = [0.1, 0.2, 0.2, 0.5]\n",
    "p2 = [0.5, 0.2, 0.1, 0.2]\n",
    "y1_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.6, 0.2, 0.1]\n",
    "p2 = [0.0, 0.5, 0.5, 0.0]\n",
    "y2_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "\n",
    "def nested_to_tensor(*l):\n",
    "    return torch.stack(list(map(torch.as_tensor, l)))\n",
    "\n",
    "\n",
    "ys_ws = nested_to_tensor(y1_ws, y2_ws, y1_ws, y2_ws, y1_ws, y2_ws, y1_ws, y2_ws) # different from 1st notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "p = [0.25, 0.25, 0.25, 0.25]\n",
    "yu_ws = [p for m in range(K)]\n",
    "yus_ws = nested_to_tensor(yu_ws, yu_ws, yu_ws, yu_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JointEntropy interface\n",
    "\n",
    "Before we look at any implementations, we want to define an interface that we want to support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class JointEntropy:\n",
    "    \"\"\"Random variables (all with the same # of categories $C$) can be added via `JointEntropy.add_variables`.\n",
    "\n",
    "    `JointEntropy.compute` computes the joint entropy.\n",
    "\n",
    "    `JointEntropy.compute_batch` computes the joint entropy of the added variables with each of the variables in the provided batch probabilities in turn.\"\"\"\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Computes the entropy of this joint entropy.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor) -> \"JointEntropy\":\n",
    "        \"\"\"Expands the joint entropy to include more terms.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_batch(self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None) -> torch.Tensor:\n",
    "        \"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.add_variables\" class=\"doc_header\"><code>JointEntropy.add_variables</code><a href=\"__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.compute\" class=\"doc_header\"><code>JointEntropy.compute</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.compute_batch\" class=\"doc_header\"><code>JointEntropy.compute_batch</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(JointEntropy.add_variables)\n",
    "show_doc(JointEntropy.compute)\n",
    "show_doc(JointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Joint Entropies\n",
    "\n",
    "To compute exact joint entropies, we have to compute all possible configurations of the $y_i$ and evaluate $p(y_1, \\dots, y_n)$ by averaging over $p(y_1, \\dots, y_n|w)$.\n",
    "\n",
    "The number of samples $M=C^N$, where $N$ is the number of variables in the joint and $C$ is the number of classes.\n",
    "\n",
    "For this, we provide a class `ExactJointEntropy` that takes $K$ and starts with no variables in the joint.\n",
    "\n",
    "### In the Paper\n",
    "![Version in the paper](batchbald_exact_joint_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class ExactJointEntropy(JointEntropy): # M = C^N, K -- number of inference samples\n",
    "    joint_probs_M_K: torch.Tensor\n",
    "\n",
    "    def __init__(self, joint_probs_M_K: torch.Tensor):\n",
    "        self.joint_probs_M_K = joint_probs_M_K\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(K: int, device=None, dtype=None) -> \"ExactJointEntropy\":\n",
    "        return ExactJointEntropy(torch.ones((1, K), device=device, dtype=dtype)) # start with empty joint entr\n",
    "\n",
    "    def compute(self) -> torch.Tensor: # entropy of these joint probs\n",
    "        probs_M = torch.mean(self.joint_probs_M_K, dim=1, keepdim=False)\n",
    "        nats_M = -torch.log(probs_M) * probs_M\n",
    "        entropy = torch.sum(nats_M) # entropy for all possible combinations\n",
    "        return entropy\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor) -> \"ExactJointEntropy\":\n",
    "        # add random variables (new N probs), extend probs -> return new joint probs including these ones\n",
    "        assert self.joint_probs_M_K.shape[1] == log_probs_N_K_C.shape[1]\n",
    "\n",
    "        N, K, C = log_probs_N_K_C.shape\n",
    "        joint_probs_K_M_1 = self.joint_probs_M_K.t()[:, :, None] #.t() -- transpose, expanded dims; will expand it with new vars\n",
    "\n",
    "        probs_N_K_C = log_probs_N_K_C.exp()\n",
    "\n",
    "        # Using lots of memory. # for each of N probs multiply joint_probs on it to recalc joint probs\n",
    "        for i in range(N): # for each object of N, ith currently\n",
    "            probs_i__K_1_C = probs_N_K_C[i][:, None, :].to(joint_probs_K_M_1, non_blocking=True) # same dtype and device as joint_probs_K_M_1\n",
    "            # non_blocking -- memory thing\n",
    "            # ith prob and expanded dim\n",
    "            joint_probs_K_M_C = joint_probs_K_M_1 * probs_i__K_1_C # joint prob mult on one by one new probs\n",
    "            joint_probs_K_M_1 = joint_probs_K_M_C.reshape((K, -1, 1)) # just included C inside M and preserve K\n",
    "\n",
    "        self.joint_probs_M_K = joint_probs_K_M_1.squeeze(2).t() # proper form of joint probs -- all M configur-s\n",
    "        # are included\n",
    "        return self\n",
    "\n",
    "    def compute_batch(self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None):\n",
    "        \"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\n",
    "        # batch vars before -- one var, one of added new vars -- another var, entropy b/w them is calculated\n",
    "        # have big tensor of probs, but not an entropy\n",
    "        assert self.joint_probs_M_K.shape[1] == log_probs_B_K_C.shape[1]\n",
    "\n",
    "        B, K, C = log_probs_B_K_C.shape # new probs from model on pool\n",
    "        M = self.joint_probs_M_K.shape[0] # already have all joint probs\n",
    "\n",
    "        if output_entropies_B is None:\n",
    "            output_entropies_B = torch.empty(B, dtype=log_probs_B_K_C.dtype, device=log_probs_B_K_C.device)\n",
    "\n",
    "        pbar = tqdm(total=B, desc=\"ExactJointEntropy.compute_batch\", leave=False)\n",
    "\n",
    "        @toma.execute.chunked(log_probs_B_K_C, initial_step=1024, dimension=0)\n",
    "        def chunked_joint_entropy(chunked_log_probs_b_K_C: torch.Tensor, start: int, end: int):\n",
    "            chunked_probs_b_K_C = chunked_log_probs_b_K_C.exp() # batch of pool probs, from log to ordinary\n",
    "            b = chunked_probs_b_K_C.shape[0]\n",
    "\n",
    "            probs_b_M_C = torch.empty(\n",
    "                (b, M, C),\n",
    "                dtype=self.joint_probs_M_K.dtype,\n",
    "                device=self.joint_probs_M_K.device,\n",
    "            ) # batch of probs including all configurations\n",
    "            for i in range(b): # iterate for batch\n",
    "                torch.matmul(\n",
    "                    self.joint_probs_M_K,\n",
    "                    chunked_probs_b_K_C[i].to(self.joint_probs_M_K, non_blocking=True),\n",
    "                    out=probs_b_M_C[i], # b -- number of elements in batch\n",
    "                ) # mult joint probs on batch of pool probs\n",
    "            probs_b_M_C /= K\n",
    "\n",
    "            output_entropies_B[start:end].copy_(\n",
    "                torch.sum(-torch.log(probs_b_M_C) * probs_b_M_C, dim=(1, 2)),\n",
    "                non_blocking=True,\n",
    "            ) # joint entropy of added vars (joint probs) and bathc of pool probs\n",
    "\n",
    "            pbar.update(end - start)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return output_entropies_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.empty\" class=\"doc_header\"><code>ExactJointEntropy.empty</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.empty</code>(**`K`**:`int`, **`device`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.add_variables\" class=\"doc_header\"><code>ExactJointEntropy.add_variables</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.compute\" class=\"doc_header\"><code>ExactJointEntropy.compute</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.compute_batch\" class=\"doc_header\"><code>ExactJointEntropy.compute_batch</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ExactJointEntropy.empty)\n",
    "show_doc(ExactJointEntropy.add_variables)\n",
    "show_doc(ExactJointEntropy.compute)\n",
    "show_doc(ExactJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479, dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log()).compute() # added new vars and computed\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.float) # just different type of data\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log()).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExactJointEntropy.compute_batch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.float)\n",
    "entropies = joint_entropy.add_variables(ys_ws[:4].log()).compute_batch(ys_ws.log()) # len(ys_ws[:4]) = 4 and not 8\n",
    "assert np.allclose(entropies, [5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362])\n",
    "entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5452, dtype=torch.float64) 5.545177444479562\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.double)\n",
    "entropies = joint_entropy.add_variables(yus_ws.log()).compute() # 0.25 tensor\n",
    "print(entropies, np.log(4) * 4)\n",
    "assert np.isclose(entropies, 5.5452, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Joint Entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute approximate joint entropies, we have to sample possible configurations of the $y_i$ from $p(y_1, \\dots, y_n|w)$ stratified by $p(w)$ and evaluate $p(y_1, \\dots, y_n)$ by averaging over $p(y_1, \\dots, y_n|w)$.\n",
    "\n",
    "The number of samples is $M$, so we use $\\frac{M}{K}$ samples per $w$.\n",
    "\n",
    "For this, we provide a class `SampledJointEntropy` that takes $K$ and $M$, and implements the 'JointEntropy' interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample, we need a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports # M/K is from following: K -- number of samples of w and for them M samples of joint prob -> for each w there are M/K joint samples\n",
    "def batch_multi_choices(probs_b_C, M: int): # what purpose?\n",
    "    \"\"\"\n",
    "    probs_b_C: Ni... x C # Ni -- current number of elems, max -- batch_size\n",
    "\n",
    "    Returns:\n",
    "        choices: Ni... x M\n",
    "    \"\"\"\n",
    "    probs_B_C = probs_b_C.reshape((-1, probs_b_C.shape[-1]))\n",
    "\n",
    "    # samples: Ni... x draw_per_xx # mb draw_per_xx == number of inference samples\n",
    "    choices = torch.multinomial(probs_B_C, num_samples=M, replacement=True) # sampling of M joint probs_B_C\n",
    "\n",
    "    choices_b_M = choices.reshape(list(probs_b_C.shape[:-1]) + [M]) # add one more dim M\n",
    "    return choices_b_M\n",
    "\n",
    "\n",
    "def gather_expand(data, dim, index):\n",
    "    if gather_expand.DEBUG_CHECKS:\n",
    "        assert len(data.shape) == len(index.shape)\n",
    "        assert all(dr == ir or 1 in (dr, ir) for dr, ir in zip(data.shape, index.shape))\n",
    "\n",
    "    max_shape = [max(dr, ir) for dr, ir in zip(data.shape, index.shape)]\n",
    "    new_data_shape = list(max_shape)\n",
    "    new_data_shape[dim] = data.shape[dim]\n",
    "\n",
    "    new_index_shape = list(max_shape)\n",
    "    new_index_shape[dim] = index.shape[dim]\n",
    "\n",
    "    data = data.expand(new_data_shape)\n",
    "    index = index.expand(new_index_shape)\n",
    "\n",
    "    return torch.gather(data, dim, index) # Gathers values along an axis specified by dim # to factorize properly mb\n",
    "\n",
    "\n",
    "gather_expand.DEBUG_CHECKS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the Paper\n",
    "![Version in the paper](batchbald_importance_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class SampledJointEntropy(JointEntropy):\n",
    "    \"\"\"Random variables (all with the same # of categories $C$) can be added via `SampledJointEntropy.add_variables`.\n",
    "\n",
    "    `SampledJointEntropy.compute` computes the joint entropy.\n",
    "\n",
    "    `SampledJointEntropy.compute_batch` computes the joint entropy of the added variables with each of the variables in the provided batch probabilities in turn.\"\"\"\n",
    "\n",
    "    sampled_joint_probs_M_K: torch.Tensor\n",
    "\n",
    "    def __init__(self, sampled_joint_probs_M_K: torch.Tensor):\n",
    "        self.sampled_joint_probs_M_K = sampled_joint_probs_M_K\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(K: int, device=None, dtype=None) -> \"SampledJointEntropy\":\n",
    "        return SampledJointEntropy(torch.ones((1, K), device=device, dtype=dtype))\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(probs_N_K_C: torch.Tensor, M: int) -> \"SampledJointEntropy\":\n",
    "        K = probs_N_K_C.shape[1]\n",
    "\n",
    "        # S: num of samples per w # factorize along each w and not mix\n",
    "        S = M // K\n",
    "\n",
    "        choices_N_K_S = batch_multi_choices(probs_N_K_C, S).long()\n",
    "\n",
    "        expanded_choices_N_1_K_S = choices_N_K_S[:, None, :, :]\n",
    "        expanded_probs_N_K_1_C = probs_N_K_C[:, :, None, :]\n",
    "\n",
    "        probs_N_K_K_S = gather_expand(expanded_probs_N_K_1_C, dim=-1, index=expanded_choices_N_1_K_S) # start of \"concat\"\n",
    "        # exp sum log seems necessary to avoid 0s\n",
    "        probs_K_K_S = torch.exp(torch.sum(torch.log(probs_N_K_K_S), dim=0, keepdim=False))\n",
    "        samples_K_M = probs_K_K_S.reshape((K, -1))\n",
    "\n",
    "        samples_M_K = samples_K_M.t()\n",
    "        return SampledJointEntropy(samples_M_K) # just a way for creating of joint_entr_M_K, and then well-known funcs from prev\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        sampled_joint_probs_M = torch.mean(self.sampled_joint_probs_M_K, dim=1, keepdim=False)\n",
    "        nats_M = -torch.log(sampled_joint_probs_M)\n",
    "        entropy = torch.mean(nats_M)\n",
    "        return entropy\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor, M2: int) -> \"SampledJointEntropy\":\n",
    "        assert self.sampled_joint_probs_M_K.shape[1] == log_probs_N_K_C.shape[1]\n",
    "\n",
    "        sample_K_M1_1 = self.sampled_joint_probs_M_K.t()[:, :, None]\n",
    "\n",
    "        new_sample_M2_K = self.sample(log_probs_N_K_C.exp(), M2).sampled_joint_probs_M_K\n",
    "        new_sample_K_1_M2 = new_sample_M2_K.t()[:, None, :]\n",
    "\n",
    "        merged_sample_K_M1_M2 = sample_K_M1_1 * new_sample_K_1_M2\n",
    "        merged_sample_K_M = merged_sample_K_M1_M2.reshape((K, -1))\n",
    "\n",
    "        self.sampled_joint_probs_M_K = merged_sample_K_M.t()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compute_batch(self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None):\n",
    "        assert self.sampled_joint_probs_M_K.shape[1] == log_probs_B_K_C.shape[1]\n",
    "\n",
    "        B, K, C = log_probs_B_K_C.shape\n",
    "        M = self.sampled_joint_probs_M_K.shape[0]\n",
    "\n",
    "        if output_entropies_B is None:\n",
    "            output_entropies_B = torch.empty(B, dtype=log_probs_B_K_C.dtype, device=log_probs_B_K_C.device)\n",
    "\n",
    "        pbar = tqdm(total=B, desc=\"SampledJointEntropy.compute_batch\", leave=False)\n",
    "\n",
    "        @toma.execute.chunked(log_probs_B_K_C, initial_step=1024, dimension=0)\n",
    "        def chunked_joint_entropy(chunked_log_probs_b_K_C: torch.Tensor, start: int, end: int):\n",
    "            b = chunked_log_probs_b_K_C.shape[0]\n",
    "\n",
    "            probs_b_M_C = torch.empty(\n",
    "                (b, M, C),\n",
    "                dtype=self.sampled_joint_probs_M_K.dtype,\n",
    "                device=self.sampled_joint_probs_M_K.device,\n",
    "            )\n",
    "            for i in range(b):\n",
    "                torch.matmul(\n",
    "                    self.sampled_joint_probs_M_K,\n",
    "                    chunked_log_probs_b_K_C[i].to(self.sampled_joint_probs_M_K, non_blocking=True).exp(),\n",
    "                    out=probs_b_M_C[i],\n",
    "                )\n",
    "            probs_b_M_C /= K\n",
    "\n",
    "            q_1_M_1 = self.sampled_joint_probs_M_K.mean(dim=1, keepdim=True)[None]\n",
    "\n",
    "            output_entropies_B[start:end].copy_(\n",
    "                torch.sum(-torch.log(probs_b_M_C) * probs_b_M_C / q_1_M_1, dim=(1, 2)) / M,\n",
    "                non_blocking=True,\n",
    "            )\n",
    "\n",
    "            pbar.update(end - start)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return output_entropies_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.empty\" class=\"doc_header\"><code>SampledJointEntropy.empty</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.empty</code>(**`K`**:`int`, **`device`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.sample\" class=\"doc_header\"><code>SampledJointEntropy.sample</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.sample</code>(**`probs_N_K_C`**:`Tensor`, **`M`**:`int`)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.compute\" class=\"doc_header\"><code>SampledJointEntropy.compute</code><a href=\"__main__.py#L40\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.add_variables\" class=\"doc_header\"><code>SampledJointEntropy.add_variables</code><a href=\"__main__.py#L46\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`, **`M2`**:`int`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.compute_batch\" class=\"doc_header\"><code>SampledJointEntropy.compute_batch</code><a href=\"__main__.py#L61\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SampledJointEntropy.empty)\n",
    "show_doc(SampledJointEntropy.sample)\n",
    "show_doc(SampledJointEntropy.compute)\n",
    "show_doc(SampledJointEntropy.add_variables)\n",
    "show_doc(SampledJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6472, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log(), 100000).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6458, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log(), 100000).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "SampledJointEntropy.compute_batch:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nattribute lookup is not defined on python value of type 'SampledJointEntropy':\n  File \"<ipython-input-43-ae113234e04b>\", line 75\n        @torch.jit.script\n        def chunked_joint_entropy(chunked_log_probs_b_K_C: torch.Tensor, start: int, end: int):            \n            M = self.sampled_joint_probs_M_K.shape[0]\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        \n            b, K, C = chunked_log_probs_b_K_C.shape[0]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b24b357fb3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjoint_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampledJointEntropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mentropies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_ws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_ws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m assert np.allclose(\n",
      "\u001b[0;32m<ipython-input-43-ae113234e04b>\u001b[0m in \u001b[0;36mcompute_batch\u001b[0;34m(self, log_probs_B_K_C, output_entropies_B)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mtoma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs_B_K_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mchunked_joint_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_log_probs_b_K_C\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_joint_probs_M_K\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_rcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0m_rcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         fn = torch._C._jit_script_compile(\n\u001b[0m\u001b[1;32m    940\u001b[0m             \u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nattribute lookup is not defined on python value of type 'SampledJointEntropy':\n  File \"<ipython-input-43-ae113234e04b>\", line 75\n        @torch.jit.script\n        def chunked_joint_entropy(chunked_log_probs_b_K_C: torch.Tensor, start: int, end: int):            \n            M = self.sampled_joint_probs_M_K.shape[0]\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        \n            b, K, C = chunked_log_probs_b_K_C.shape[0]\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.float)\n",
    "entropies = joint_entropy.add_variables(ys_ws[:4].log(), 10000).compute_batch(ys_ws.log())\n",
    "\n",
    "print(entropies)\n",
    "assert np.allclose(\n",
    "    entropies,\n",
    "    [5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362],\n",
    "    atol=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamically chooses JointEntropy Method\n",
    "\n",
    "Finally, we want to be able to dynamically pick either class depending on the maximum number of samples we want. (And also resample if necessary as we add variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class DynamicJointEntropy(JointEntropy):\n",
    "    inner: JointEntropy\n",
    "    log_probs_max_N_K_C: torch.Tensor\n",
    "    N: int\n",
    "    M: int\n",
    "\n",
    "    def __init__(self, M: int, max_N: int, K: int, C: int, dtype=None, device=None):\n",
    "        self.M = M\n",
    "        self.N = 0\n",
    "        self.max_N = max_N\n",
    "\n",
    "        self.inner = ExactJointEntropy.empty(K, dtype=dtype, device=device)\n",
    "        self.log_probs_max_N_K_C = torch.empty((max_N, K, C), dtype=dtype, device=device)\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor) -> \"DynamicJointEntropy\":\n",
    "        C = self.log_probs_max_N_K_C.shape[2]\n",
    "        add_N = log_probs_N_K_C.shape[0]\n",
    "\n",
    "        assert self.log_probs_max_N_K_C.shape[0] >= self.N + add_N\n",
    "        assert self.log_probs_max_N_K_C.shape[2] == C\n",
    "\n",
    "        self.log_probs_max_N_K_C[self.N : self.N + add_N] = log_probs_N_K_C\n",
    "        self.N += add_N\n",
    "\n",
    "        num_exact_samples = C ** self.N\n",
    "        if num_exact_samples > self.M:\n",
    "            self.inner = SampledJointEntropy.sample(self.log_probs_max_N_K_C[: self.N].exp(), self.M)\n",
    "        else:\n",
    "            self.inner.add_variables(log_probs_N_K_C)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.inner.compute()\n",
    "\n",
    "    def compute_batch(self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None) -> torch.Tensor:\n",
    "        \"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\n",
    "        return self.inner.compute_batch(log_probs_B_K_C, output_entropies_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DynamicJointEntropy.add_variables\" class=\"doc_header\"><code>DynamicJointEntropy.add_variables</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DynamicJointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DynamicJointEntropy.compute_batch\" class=\"doc_header\"><code>DynamicJointEntropy.compute_batch</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DynamicJointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DynamicJointEntropy.add_variables)\n",
    "show_doc(DynamicJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6479, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = DynamicJointEntropy(256, 8, K, 4, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log()).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(joint_entropy.inner) == ExactJointEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.1794, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "entropy = joint_entropy.add_variables(ys_ws[4:].log()).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 9.2756, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(joint_entropy.inner) == SampledJointEntropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
